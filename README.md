# DevOps_Takima
Formation en DevOps de Takima

**1-1 For which reason is it better to run the container with a flag -e to give the environment variables rather than put them directly in the Dockerfile?**
It's generally better to use the -e flag when running a container to set environment variables, rather than setting them directly in the Dockerfile, for several reasons: 

* Flexibility: With the -e flag, you can set environment variables dynamically during the run-time of the container, without having to modify the Dockerfile. This allows for more flexibility and makes it easier to manage different environments or configurations.
* Separation of Concerns: By separating the configuration of the container from its runtime settings, you can make it easier to understand and maintain your Dockerfiles. You can write a separate file or script that sets environment variables using the -e flag, making it easier to manage and update different environments.
* Reusability: With Dockerfile, you have to recreate the entire image every time you want to set a new environment variable. Using the -e flag, you can reuse an existing container with a specific configuration, which makes it more efficient.
* Testing: It's often easier to test different environments or configurations by modifying the environment variables at run-time, rather than rebuilding and redeploying the entire image.
     

However, there are some scenarios where setting environment variables directly in the Dockerfile might be preferred: 

* Image Integrity: If an application requires specific environment variables to function correctly, it's often better to set them directly in the Dockerfile, so that they're always present when the container is run.
* Immutable Images: When using immutable images (where the Docker image is never modified), setting environment variables in the Dockerfile ensures that the image remains consistent and predictable.
     

In summary, while there are cases where setting environment variables directly in the Dockerfile might be preferred, using the -e flag provides more flexibility and makes it easier to manage different environments and configurations. 

**1-2 Why do we need a volume to be attached to our postgres container?**

You need a volume to be attached to your Postgres container for several reasons: 

* Persistence: By mounting a volume to the container, you can persist data even after the container is deleted or restarted. This ensures that your database schema and data are not lost when the container is recreated.
* Data Volume: A volume allows you to separate your application's data from the container itself. This makes it easier to manage and maintain your data independently of the container.
* Isolation: Attaching a volume to the container provides isolation between containers, which helps prevent data corruption or overwriting when multiple containers are running concurrently.
     Easy Maintenance: With a volume attached, you can easily back up or restore your database data by attaching another volume with the desired data.
     

Some common use cases for volumes with Postgres include: 

* Development environments: Attaching a volume allows developers to persist data between development cycles, making it easier to work on complex applications.
* Database backup and recovery: Volumes can be used to back up and restore your database data, ensuring that your application is always available.
* Multi-container setups: When running multiple containers together (e.g., Postgres, Nginx, and a web app), volumes ensure that each container has its own dedicated storage space.
     

Best practices for attaching volumes to Postgres include: 

* Use a named volume: Attach the volume to a specific name, making it easier to reference in your Dockerfiles or configurations.
* Specify data directory: Make sure to specify the correct data directory path when mounting the volume, as this determines where data will be stored.
* Monitor and manage storage space: Regularly monitor and manage the storage space allocated to the volume to prevent running out of disk space.
     
**1-3 Document your database container essentials: commands and Dockerfile.**

This document outlines the essential commands and configuration for a Postgres database container. 

Commands 

* Start the container: docker run -d --name my-postgres postgres
  * This command starts a new Postgres container with the name my-postgres.
         
* Connect to the container: docker exec -it my-postgres /bin/bash
  *This command connects to the running Postgres container and opens a shell.
         
* Initialize the database: psql -U postgres
  * This command connects to the Postgres database as the superuser (postgres) and initializes the database schema.
         
* Create a new user and database: CREATE USER myuser WITH PASSWORD 'mypassword'; CREATE DATABASE mydb; GRANT ALL PRIVILEGES ON DATABASE mydb TO myuser;
  *These commands create a new user (myuser) with the specified password, creates a new database (mydb), and grants all privileges to the user.
         
* Exit the container: exit
  *This command exits the connected shell.    
     
Volume Configuration 

To persist data even after the container is deleted or restarted, we use a named volume to store the Postgres database 

This volume is mounted at /var/lib/postgresql/data inside the container and persists across container restarts. 

Tips and Variations 

* To reset the database to its default schema, run psql -U postgres --command="DROP SCHEMA public;" and then psql -U postgres --command="CREATE SCHEMA public;"
* To use a different Postgres version or image, modify the FROM line in the Dockerfile (e.g., FROM postgres:9.6)
* To expose additional ports for your application, add more EXPOSE statements to the Dockerfile
* To enable SSL/TLS encryption, create a new certificate and private key file using the openssl command and mount them as volumes
     

Security Considerations 

 * Use strong passwords for all users and databases.
 * Limit access to sensitive data by granting privileges only to necessary users.
 * Regularly update Postgres and your application to prevent known vulnerabilities.

   **Why Multistage Builds?**

A multistage build is necessary when you have multiple stages of your application that require different environments or dependencies to be built. Here are some reasons why: 

* Layer caching: In a multistage build, each stage creates its own layer in the Docker image. This means that if a particular dependency changes between two stages, only the new layer needs to be updated, rather than re-downloading all previous layers.
* Reduced image size: By separating the builds into different stages, you can avoid including unnecessary dependencies or code that is not needed for subsequent stages.
* Improved build efficiency: Multistage builds allow you to parallelize your build process, which can significantly improve the time it takes to compile and package your application.


**1-5 Why do we need a reverse proxy?**

A reverse proxy is a server that sits between client devices and backend servers, and it forwards client requests to those servers. Here's why we need a reverse proxy, with 5 key reasons:

1. Load Balancing

A reverse proxy can distribute incoming traffic across multiple backend servers to prevent any single server from being overwhelmed. This improves performance and reliability.
2. Security and Anonymity

It hides the identity and structure of backend servers from the outside world, adding a layer of protection against attacks (like DDoS). It can also enforce security policies like IP whitelisting or rate limiting.
3. SSL Termination

Handling SSL/TLS encryption at the reverse proxy level offloads the CPU-intensive work from backend servers, simplifying certificate management and improving performance.
4. Caching and Compression

It can cache static content (like images or CSS files) and compress responses before sending them to clients, reducing load times and saving bandwidth.
5. Centralized Authentication and Logging

A reverse proxy can manage authentication, logging, and access control for multiple services, making it easier to maintain and audit.

**1-6 Why is docker-compose so important?**
It is realy importante cause it will manadge all our docker container, he will create them and link them. that allow an easy deployement on every machine.

**1-7 Document docker-compose most important commands.** 
ðŸš€ Basic Lifecycle Commands
Command	Description
* docker-compose up	Starts containers as defined in docker-compose.yml. Builds images if needed.
* docker-compose up -d	Same as above, but runs containers in the background (detached mode).
* docker-compose down	Stops and removes all containers, networks, and volumes created by up.
* docker-compose stop	Stops the running containers but does not remove them.
* docker-compose start	Starts previously stopped containers.
* docker-compose restart	Restarts all containers.
ðŸ›  Build & Configuration
* Command	Description
* docker-compose build	Builds images defined in the compose file.
* docker-compose build --no-cache	Builds images without using cache.
* docker-compose config	Validates and displays the full configuration (resolves variables, etc.).
ðŸ‘€ Viewing and Debugging
Command	Description
* docker-compose ps	Lists the running containers managed by this docker-compose.yml.
* docker-compose logs	Shows logs from containers.
* docker-compose logs -f	Shows logs and follows output (like tail -f).
ðŸ”§ Executing Commands
Command	Description
* docker-compose exec SERVICE CMD	Executes a command inside a running container. Example: docker-compose exec web bash
* docker-compose run SERVICE CMD	Runs a one-off command in a new container. Does not use volumes from previous up.
ðŸ§¹ Cleanup
Command	Description
* docker-compose down -v	Removes volumes along with containers and networks.
* docker-compose rm	Removes stopped containers created by compose.

creadential : Ollama3.3
**1-8 Document your docker-compose file**

1. Services Section

The services: key defines all the containers (services) that will be managed together.
2. database Service

database:
  build: "database"
  environment:
    - POSTGRES_PASSWORD=pwd
  volumes:
    - db-volume:/var/lib/postgresql/data
  networks:
    - my-network

Explanation:

    build: "database"
    Builds the image using the Dockerfile located in the ./database/ directory.

    environment:
    Sets environment variables. POSTGRES_PASSWORD=pwd is required by the official PostgreSQL image.

    volumes:
    Mounts a named volume (db-volume) to persist database data.

    networks:
    Connects this service to the custom network my-network.

3. simple-api Service

simple-api:
  build: "simple-api"
  environment:
    - DATABASE_HOST=database
    - DATABASE_PASSWORD=pwd
  networks:
    - my-network

Explanation:

    build: "simple-api"
    Builds the image from the ./simple-api/ directory.

    environment:

        DATABASE_HOST=database: Connects to the database service (via Docker DNS).

        DATABASE_PASSWORD=pwd: Password to authenticate with the database.

    networks:
    Connects to the shared network for communication with database.

4. httpd Service

httpd:
  build: "http-server"
  ports:
    - "80:80"
  networks:
    - my-network

Explanation:

    build: "http-server"
    Builds from ./http-server/ directory (presumably an Apache or Nginx config).

    ports: "80:80"
    Maps container port 80 to host port 80, making it accessible from your browser.

    networks:
    Also connected to the my-network for internal communication if needed.

5. Networks Section

networks:
  my-network:

Explanation:

    Creates a custom user-defined bridge network named my-network.

    Allows services to communicate by service name (like database).

6. Volumes Section

volumes:
  db-volume:

Explanation:

    Declares a named volume called db-volume.

    Used by the database service to persist PostgreSQL data even after container shutdown.

7. Internal Communication

All services (database, simple-api, httpd) share the same network (my-network), allowing them to communicate securely via internal DNS (database, simple-api, etc.).
8. Security Note

You're currently hardcoding the database password (pwd) in the docker-compose.yml.
Recommended: Use Docker secrets or .env files to manage credentials securely.

Example using .env:

environment:
  - POSTGRES_PASSWORD=${DB_PASSWORD}

And in .env file:

DB_PASSWORD=supersecurepwd


**1-9 Document your publication commands and published images in dockerhub.**
We have 3 image on our docker hub, one for each container, there allow an easy pull on every place on earth 
their name are : 
* tp-devops-httpd
* tp-devops-simple-api
* tp-devops-database

**1-10 Why do we put our images into an online repo?**
Benefit	Description
* Distribution	Easily share across teams, machines, or the internet
* CI/CD	Automate deployment pipelines
* Versioning	Track and rollback image versions
* Storage	Persistent and central image store
* Security	Control access, scan for vulnerabilities
* Cloud Ready	Compatible with cloud/container orchestration

**2-1 What are testcontainers?**

Testcontainers is a library that allows you to use Docker containers in your automated tests, primarily for integration and end-to-end testing.
In simple terms:

Testcontainers lets you spin up real instances of databases, message brokers, web servers, etc., in Docker containers as part of your test suiteâ€”so you can test against actual services instead of mocks.
Why use Testcontainers?

* Realistic Testing
* Test against real versions of services like PostgreSQL, Redis, Kafka, etc., ensuring your code works in production-like environments.

* Isolation
* Each test can run with a clean instance of a service, reducing test flakiness due to shared state.

* Automation
* No need to manually set up and tear down servicesâ€”Testcontainers handles that for you.

* Cross-language Support
* 
Originally for Java, now also available in other languages:

* Java: testcontainers-java

* Python: testcontainers-python

* Node.js: testcontainers-node

**2-2 For what purpose do we need to use secured variables ?**
* Protect Sensitive Information
* Secrets help store and access confidential data like:
* Prevent Credential Leaks By keeping these values out of the source code and workflow YAML files, you reduce the risk of:
* Controlled Access

**2-3 Why did we put needs: build-and-test-backend on this job? Maybe try without this and you will see!**
we do this so we can make the first tasks befor this one and see if the first tasks work. it allow us to see easly where is the bug 

**2-4 For what purpose do we need to push docker images?**
we need to push docker images so there are on our dockerHub and we can pull them on other machine and have an acces to it for the implementation



**3-1 Document your inventory and base commands**
Our inventorie containe the setup who indicate the place of the ssh key and name of the host

**3-2 Document your playbook**
The playbook has first the roll to install docker and follow the step of installation.
After we modify it, his roles is to start the tasks and roles we build in the folder ansible
  
**3-3 Document your docker_container tasks configuration**
tasks is empty but our work is in roles, we have a folder by tasks, each foler containe a folder tasks and a main.yml.
the tasks folder allow our ansible to find the main.yml and the main.yml contain the instruction of job.
we have 5 roles, one to create a network, 3 to create our docker container, 1 to install docker 